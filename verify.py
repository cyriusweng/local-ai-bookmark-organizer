import os
from bs4 import BeautifulSoup

# --- é…ç½® ---
BASE_DIR = '/Users/cyriusweng/0-Inbox/bookmarks'
ORIGINAL_FILE = os.path.join(BASE_DIR, 'bookmarks.html')     # åŸå§‹æ–‡ä»¶ (Step 1 çš„è¾“å…¥)
FINAL_FILE = os.path.join(BASE_DIR, 'final_bookmarks.html')  # æœ€ç»ˆæ–‡ä»¶ (Step 5 çš„è¾“å‡º)

def normalize_url(url):
    """
    æ¸…æ´— URL ä»¥é˜²æ­¢å› æœ«å°¾æ–œæ å¯¼è‡´çš„è¯¯åˆ¤
    ä¾‹å¦‚: 'http://google.com' å’Œ 'http://google.com/' åº”è§†ä¸ºåŒä¸€ä¸ª
    """
    if not url: return ""
    u = url.strip()
    if u.endswith('/'):
        return u[:-1]
    return u

def extract_urls(filepath):
    """ä» HTML æ–‡ä»¶ä¸­æå–æ‰€æœ‰ URL"""
    if not os.path.exists(filepath):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return set(), 0
    
    print(f"ğŸ“– æ­£åœ¨è¯»å–: {os.path.basename(filepath)} ...")
    with open(filepath, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'html.parser')
    
    links = soup.find_all('a')
    
    # ä½¿ç”¨é›†åˆ (Set) æ¥å­˜å‚¨å”¯ä¸€çš„ URL
    unique_urls = set()
    total_count = 0
    
    for link in links:
        href = link.get('href')
        if href and href.startswith('http'):
            norm_url = normalize_url(href)
            unique_urls.add(norm_url)
            total_count += 1
            
    return unique_urls, total_count

def run_verification():
    print("-" * 40)
    print("ğŸ” å¯åŠ¨å®Œæ•´æ€§æ ¡éªŒç¨‹åº")
    print("-" * 40)

    # 1. æå–åŸå§‹æ•°æ®
    orig_set, orig_total = extract_urls(ORIGINAL_FILE)
    print(f"âœ… åŸå§‹æ–‡ä»¶: æ€»é“¾æ¥ {orig_total} ä¸ª | å»é‡åå”¯ä¸€é“¾æ¥ {len(orig_set)} ä¸ª")

    # 2. æå–æ–°æ•°æ®
    final_set, final_total = extract_urls(FINAL_FILE)
    print(f"âœ… æ–°ç”Ÿæˆæ–‡ä»¶: æ€»é“¾æ¥ {final_total} ä¸ª | å»é‡åå”¯ä¸€é“¾æ¥ {len(final_set)} ä¸ª")

    print("-" * 40)

    # 3. æ ¸å¿ƒæ¯”å¯¹é€»è¾‘ (é›†åˆè¿ç®—)
    # ä¸¢å¤±çš„ = åŸæœ‰çš„å”¯ä¸€é“¾æ¥ - ç°æœ‰çš„å”¯ä¸€é“¾æ¥
    missing_urls = orig_set - final_set
    
    # 4. è¾“å‡ºæŠ¥å‘Š
    if len(missing_urls) == 0:
        print("ğŸ‰ å®Œç¾ï¼æ²¡æœ‰ä¸¢å¤±ä»»ä½•æ•°æ®ã€‚")
        print("   æ‰€æœ‰åŸå§‹é“¾æ¥éƒ½å·²å­˜åœ¨äºæ–°æ–‡ä»¶ä¸­ã€‚")
    else:
        print(f"âš ï¸  è­¦å‘Š: å‘ç° {len(missing_urls)} ä¸ªé“¾æ¥åœ¨è½¬æ¢è¿‡ç¨‹ä¸­ä¸¢å¤±ï¼")
        print("   ä¸¢å¤±åˆ—è¡¨å¦‚ä¸‹:")
        for i, url in enumerate(missing_urls, 1):
            print(f"   {i}. {url}")
            
    print("-" * 40)
    
    # 5. é¢å¤–æ£€æŸ¥ï¼šæœ‰æ²¡æœ‰æ–°å¢åŠ çš„ï¼Ÿ(é€šå¸¸ä¸åº”è¯¥æœ‰ï¼Œé™¤é AI å¹»è§‰æˆ–è€…æ˜¯çº é”™äº† URL)
    added_urls = final_set - orig_set
    if len(added_urls) > 0:
        print(f"â„¹ï¸  æç¤º: æ–°æ–‡ä»¶æ¯”æ—§æ–‡ä»¶å¤šå‡ºäº† {len(added_urls)} ä¸ªå”¯ä¸€é“¾æ¥ (å¯èƒ½æ˜¯ URL æ ¼å¼åŒ–å·®å¼‚å¯¼è‡´)ã€‚")

if __name__ == "__main__":
    run_verification()
