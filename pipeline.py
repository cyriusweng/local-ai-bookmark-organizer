import os
import json
import argparse
import time
from bs4 import BeautifulSoup
import ollama
from tqdm import tqdm
import urllib3
from urllib.parse import urlparse

# å¼•å…¥æŠ—æŒ‡çº¹æµè§ˆå™¨åº“
from curl_cffi import requests as crequests

# --- 1. å…¨å±€é…ç½® (Configuration) ---
BASE_DIR = '/Users/cyriusweng/0-Inbox/bookmarks'
INPUT_HTML = os.path.join(BASE_DIR, 'bookmarks.html')

FILE_RAW = os.path.join(BASE_DIR, '1_raw.json')
FILE_ENRICHED = os.path.join(BASE_DIR, '2_enriched.json')
FILE_TAGGED = os.path.join(BASE_DIR, '3_tagged.json')
FILE_CATEGORIZED = os.path.join(BASE_DIR, '4_categorized.json')
OUTPUT_FINAL = os.path.join(BASE_DIR, 'final_bookmarks.html')

MODEL_NAME = 'qwen2.5:3b'
IMPERSONATE_BROWSER = "chrome124"
TIMEOUT = 15

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- å·¥å…·å‡½æ•°ï¼šæå–åŸŸå ---
def get_domain(url):
    try:
        domain = urlparse(url).netloc
        return domain if domain else "Unknown Domain"
    except:
        return "Invalid URL"

# --- æ¨¡å— 1: æ‘„å…¥ (Ingestion) ---
def step1_ingestion():
    print("ğŸš© [Step 1] å¼€å§‹æ‘„å…¥ä¹¦ç­¾...")
    if not os.path.exists(INPUT_HTML):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {INPUT_HTML}")
        return

    with open(INPUT_HTML, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'html.parser')

    items = soup.find_all('a')
    bookmarks = []
    
    print(f"ğŸ” æ‰«æåˆ° {len(items)} ä¸ªé“¾æ¥ã€‚æ­£åœ¨æ¸…æ´—æ•°æ®...")
    for item in items:
        url = item.get('href')
        if url and url.startswith('http'):
            bookmarks.append({
                "id": len(bookmarks),
                "url": url,
                "user_title": item.text.strip(),
            })
            
    with open(FILE_RAW, 'w', encoding='utf-8') as f:
        json.dump(bookmarks, f, indent=2, ensure_ascii=False)
    print(f"âœ… [Step 1] å®Œæˆã€‚å·²ä¿å­˜è‡³ {FILE_RAW}")


# --- æ¨¡å— 2: æµ“ç¼© (Enrichment) ---
def step2_enrichment():
    print("ğŸš© [Step 2] å¼€å§‹é«˜é²æ£’æ€§æŠ“å–...")
    if not os.path.exists(FILE_RAW):
        print("âŒ è¯·å…ˆè¿è¡Œ Step 1")
        return

    with open(FILE_RAW, 'r', encoding='utf-8') as f:
        bookmarks = json.load(f)

    SOFT_404_INDICATORS = ["domain for sale", "domain expired", "404 not found", "page not found", "godaddy"]

    for bm in tqdm(bookmarks, desc="Fetching Metadata"):
        if 'status' in bm and bm['status'] != 'pending': continue

        try:
            response = crequests.get(bm['url'], impersonate=IMPERSONATE_BROWSER, timeout=TIMEOUT, verify=False)
            
            if response.status_code in [404, 410]:
                bm['status'] = 'dead'
            elif response.status_code in [401, 403, 406, 429, 503]:
                bm['status'] = 'alive_but_blocked'
                bm['site_title'] = bm['user_title']
            elif 200 <= response.status_code < 300:
                if response.encoding is None: response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                title = soup.title.string.strip() if soup.title else ""

                if any(ind in title.lower() for ind in SOFT_404_INDICATORS):
                    bm['status'] = 'dead'
                else:
                    bm['status'] = 'alive'
                    bm['site_title'] = title
                    meta_keys = soup.find('meta', attrs={'name': 'keywords'})
                    bm['seo_keywords'] = meta_keys.get('content', '')[:800] if meta_keys else ""
                    meta_desc = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', attrs={'property': 'og:description'})
                    bm['seo_description'] = meta_desc.get('content', '')[:800] if meta_desc else ""
            else:
                bm['status'] = f'unknown_{response.status_code}'
        except Exception as e:
            bm['status'] = 'error'
            bm['error_detail'] = str(e)[:30]

        if bm['id'] % 10 == 0:
            with open(FILE_ENRICHED, 'w', encoding='utf-8') as f:
                json.dump(bookmarks, f, indent=2, ensure_ascii=False)

    with open(FILE_ENRICHED, 'w', encoding='utf-8') as f:
        json.dump(bookmarks, f, indent=2, ensure_ascii=False)
    print(f"âœ… [Step 2] å®Œæˆã€‚")


# --- æ¨¡å— 3: å…¨ç»´åº¦é€è§† (Analysis - Maximum Information Density) ---
def step3_analysis():
    print(f"ğŸš© [Step 3] AI å…¨ç»´åº¦æ·±åº¦åˆ†æ (é€‚é… Dolphin-Llama3 ç›´å‡ºæ¨¡å¼)...")
    if not os.path.exists(FILE_ENRICHED):
        print("âŒ è¯·å…ˆè¿è¡Œ Step 2")
        return

    with open(FILE_ENRICHED, 'r', encoding='utf-8') as f:
        bookmarks = json.load(f)

    for bm in tqdm(bookmarks, desc="AI Processing"):
        if 'ai_tags' in bm: continue

        domain_fallback = f"Domain: {get_domain(bm['url'])}"
        
        # é€»è¾‘ä¿ç•™ï¼šæ­»é“¾ä½†æœ‰ç”¨æˆ·å‘½åçš„ï¼Œä¾ç„¶å°è¯•åˆ†æ
        if bm.get('status') == 'dead' and len(bm['user_title']) < 5:
            bm['ai_tags'] = f"Dead Link, {domain_fallback}"
            continue

        print(f"\n[æ­£åœ¨åˆ†æ]: {bm['user_title']}")
        
        # ä¾ç„¶æ„å»ºé«˜å¯†åº¦ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿ Dolphin ä¹Ÿèƒ½çœ‹åˆ°è¿™äº›ä¿¡æ¯
        context_input = f"""
        - User's Bookmark Title: {bm['user_title']}
        - Page Meta Title: {bm.get('site_title', 'N/A')}
        - URL Structure: {bm['url']}
        - SEO Keywords: {bm.get('seo_keywords', 'N/A')}
        - Content Summary: {bm.get('seo_description', 'N/A')}
        """
        
        # --- ä¿®æ”¹ç‚¹ 1: é’ˆå¯¹ Dolphin-Llama3 çš„ Prompt ä¼˜åŒ– ---
        # ç§»é™¤äº†æ‰€æœ‰ <think> ç›¸å…³çš„æŒ‡ä»¤ï¼Œæ”¹ä¸ºæ ‡å‡†çš„æŒ‡ä»¤è·Ÿéšæ ¼å¼
        prompt = f"""
        You are a smart bookmark organizer.
        
        [Task]
        Identify the fundamental essence of this bookmark based on the provided context.
        
        [Context Information]
        {context_input}
        
        [Rules]
        1. **Crucial**: Trust the 'User's Bookmark Title' the most. It reveals the user's specific intent.
        2. **Crucial**: Output **ONLY** the tags, separated by commas!!!
        3. MUST NOT output any introductory text, explanations, or "Here are the tags". Output ONLY JUST the tags.
        4. Never output like: "Based on the provided context information, I would categorize this bookmark as follows:", or "And here are the corresponding tags:".
        
        [Example Output]
        Python, Data Visualization, Matplotlib
        """

        try:
            # Dolphin ç³»åˆ—é€šå¸¸å¾ˆå¬è¯ï¼Œä¸ä½¿ç”¨ stream ä¹Ÿå¯ä»¥ï¼Œä½†ä¿ç•™ stream å¯ä»¥è®©ä½ çœ‹åˆ°è¿›åº¦
            stream = ollama.chat(
                model=MODEL_NAME, 
                messages=[{'role': 'user', 'content': prompt}],
                stream=True
            )
            
            full_response = ""
            for chunk in stream:
                content = chunk['message']['content']
                full_response += content
                print(content, end='', flush=True) # å®æ—¶æ‰“å° tagsï¼Œæ–¹ä¾¿ç¡®è®¤
            print("") # æ¢è¡Œ
            
            # --- ä¿®æ”¹ç‚¹ 2: ç®€åŒ–çš„è§£æé€»è¾‘ ---
            # ç›´æ¥æ¸…ç†é¦–å°¾ç©ºç™½å’Œå¯èƒ½çš„å¼•å·
            result = full_response.strip().replace('"', '').replace("'", "")
            
            # ç®€å•çš„è„æ•°æ®è¿‡æ»¤
            if len(result) < 2 or "sorry" in result.lower() or "cannot" in result.lower():
                bm['ai_tags'] = domain_fallback
            else:
                bm['ai_tags'] = result
                
        except Exception as e:
            print(f"âŒ Error: {e}")
            bm['ai_tags'] = domain_fallback

        # æ¯ 10 æ¡ä¿å­˜ä¸€æ¬¡ (ä¸éœ€è¦åƒ R1 é‚£ä¹ˆé¢‘ç¹äº†ï¼Œå› ä¸ºé€Ÿåº¦ä¼šå¿«å¾ˆå¤š)
        if bm['id'] % 10 == 0:
            with open(FILE_TAGGED, 'w', encoding='utf-8') as f:
                json.dump(bookmarks, f, indent=2, ensure_ascii=False)

    with open(FILE_TAGGED, 'w', encoding='utf-8') as f:
        json.dump(bookmarks, f, indent=2, ensure_ascii=False)
    print(f"âœ… [Step 3] å®Œæˆã€‚")

# --- æ¨¡å— 3.5: ä¸Šå¸è§†è§’ (Taxonomy Generation) ---
# è¿™ä¸€æ­¥æ˜¯è¿æ¥ "å¾®è§‚æ ‡ç­¾" å’Œ "å®è§‚åˆ†ç±»" çš„æ¡¥æ¢
def step3_5_taxonomy_gen():
    print(f"ğŸš© [Step 3.5] AI æ„å»ºåŠ¨æ€åˆ†ç±»ä½“ç³» (åŸºäº {MODEL_NAME})...")
    if not os.path.exists(FILE_TAGGED):
        print("âŒ è¯·å…ˆè¿è¡Œ Step 3")
        return

    import collections
    
    # 1. Python è´Ÿè´£è„æ´»ï¼šæ”¶é›†ã€æ¸…æ´—ã€å»é‡ã€ç»Ÿè®¡é¢‘ç‡
    with open(FILE_TAGGED, 'r', encoding='utf-8') as f:
        bookmarks = json.load(f)

    all_tags = []
    print("   -> æ­£åœ¨èšåˆæ‰€æœ‰æ ‡ç­¾...")
    for bm in bookmarks:
        if 'ai_tags' in bm and "Domain:" not in bm['ai_tags']:
            # åˆ†å‰²ã€å»ç©ºã€è½¬å°å†™ä»¥ä¾¿ç»Ÿè®¡ï¼ˆä½†ä¿ç•™åŸæ ¼å¼ç”¨äºå±•ç¤ºï¼‰
            tags = [t.strip() for t in bm['ai_tags'].split(',') if len(t.strip()) > 1]
            all_tags.extend(tags)

    # ç»Ÿè®¡é¢‘ç‡
    tag_counts = collections.Counter(all_tags)
    unique_tags_count = len(tag_counts)
    print(f"   -> å…±å‘ç° {len(all_tags)} ä¸ªæ ‡ç­¾ï¼Œå…¶ä¸­å»é‡åæœ‰ {unique_tags_count} ä¸ªå”¯ä¸€æ ‡ç­¾ã€‚")

    # ç­–ç•¥ï¼šå– Top 500 é«˜é¢‘æ ‡ç­¾ + éšæœº 100 ä¸ªä½é¢‘æ ‡ç­¾ä½œä¸ºæ ·æœ¬ï¼Œ
    # æ—¢ä¿è¯æ ¸å¿ƒåˆ†ç±»å‡†ç¡®ï¼Œåˆç…§é¡¾åˆ°é•¿å°¾å†…å®¹ï¼Œé˜²æ­¢ token æº¢å‡ºï¼ˆè™½ç„¶ Qwen æ”¯æŒé•¿æ–‡æœ¬ï¼Œä½†å¤ªé•¿ä¼šå½±å“æ¨ç†è´¨é‡ï¼‰
    most_common = [t[0] for t in tag_counts.most_common(500)]
    
    # æ„å»º Prompt context
    tags_block = ", ".join(most_common)
    
    print("   -> æ­£åœ¨è¯·æ±‚ AI å½’çº³åˆ†ç±»æ ‘ (è¿™å¯èƒ½éœ€è¦ 1-2 åˆ†é’Ÿ)...")
    
    prompt = f"""
    [Context]
    I have a collection of browser bookmarks tagged with the following keywords. 
    These are the most frequent tags used in my collection:
    
    {tags_block}
    
    [Task]
    Analyze these tags to understand the user's interests and work domains.
    Create a Hierarchical Taxonomy (Classification System) that covers these topics.
    
    [Requirements]
    1. Summarize these into 10-15 High-Level Categories (Level 1).
    2. For each Level 1 category, provide 3-6 distinct Sub-Categories (Level 2).
    3. The system must be MECE (Mutually Exclusive, Collectively Exhaustive).
    4. Keep category names professional, concise, and academic (English).
    
    [Output Format]
    Return ONLY a JSON object. No markdown formatting, no explanations.
    Structure:
    {{
        "Taxonomy": {{
            "Category Name 1": ["Subcat A", "Subcat B", "Subcat C"],
            "Category Name 2": ["Subcat X", "Subcat Y"]
        }}
    }}
    """

    try:
        response = ollama.chat(
            model=MODEL_NAME, # æ­¤æ—¶åº”ä¸º qwen2.5:3b
            messages=[{'role': 'user', 'content': prompt}],
            options={'temperature': 0.2} # ä½æ¸©åº¦ï¼Œä¿è¯é€»è¾‘ä¸¥å¯†
        )
        
        content = response['message']['content']
        # æ¸…æ´—å¯èƒ½å­˜åœ¨çš„ Markdown ä»£ç å—æ ‡è®°
        content = content.replace("```json", "").replace("```", "").strip()
        
        taxonomy = json.loads(content)
        
        # ä¿å­˜è¿™ä¸ªç”Ÿæˆçš„åˆ†ç±»æ ‘ï¼Œä¾› Step 4 ä½¿ç”¨
        TAXONOMY_FILE = os.path.join(BASE_DIR, 'taxonomy_config.json')
        with open(TAXONOMY_FILE, 'w', encoding='utf-8') as f:
            json.dump(taxonomy, f, indent=2, ensure_ascii=False)
            
        print(f"âœ… [Step 3.5] åˆ†ç±»æ ‘æ„å»ºå®Œæˆï¼å·²ä¿å­˜è‡³ {TAXONOMY_FILE}")
        print("   -> é¢„è§ˆç”Ÿæˆçš„é¡¶çº§åˆ†ç±»:")
        for key in taxonomy.get("Taxonomy", {}).keys():
            print(f"      - {key}")

    except Exception as e:
        print(f"âŒ Error extracting taxonomy: {e}")
        print(f"Raw output was: {content[:100]}...")


# --- æ¨¡å— 4: æ¶æ„ (Architecture - åŠ¨æ€åˆ†ç±»ç‰ˆ) ---
def step4_categorization():
    print(f"ğŸš© [Step 4] AI å½’ç±»æ‰§è¡Œ (åŸºäºåŠ¨æ€ç”Ÿæˆçš„åˆ†ç±»æ ‘)...")
    
    TAXONOMY_FILE = os.path.join(BASE_DIR, 'taxonomy_config.json')
    if not os.path.exists(FILE_TAGGED):
        print("âŒ è¯·å…ˆè¿è¡Œ Step 3")
        return
    
    # è¯»å– AI åœ¨ Step 3.5 ç”Ÿæˆçš„åˆ†ç±»æ ‘
    custom_taxonomy = {}
    if os.path.exists(TAXONOMY_FILE):
        with open(TAXONOMY_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            custom_taxonomy = data.get("Taxonomy", {})
            print("   -> å·²åŠ è½½è‡ªå®šä¹‰åˆ†ç±»æ ‘ã€‚")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°åˆ†ç±»æ ‘æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ Step 3.5ï¼(æˆ–è€…åœ¨æ­¤å¤„å›é€€åˆ°ç¡¬ç¼–ç åˆ—è¡¨)")
        return

    # å°†å­—å…¸è½¬æ¢ä¸º Prompt å‹å¥½çš„å­—ç¬¦ä¸²
    taxonomy_str = json.dumps(custom_taxonomy, indent=2)

    with open(FILE_TAGGED, 'r', encoding='utf-8') as f:
        bookmarks = json.load(f)

    for bm in tqdm(bookmarks, desc="Categorizing"):
        if 'category' in bm: continue 

        tags = bm.get('ai_tags', '')
        
        # ç®€å•é€»è¾‘ï¼šæ­»é“¾å’Œçº¯åŸŸåå›é€€
        if "Domain:" in tags:
            bm['category'] = "Unsorted Websites"
            bm['subcategory'] = tags.split("Domain:")[-1].strip()
            continue

        prompt = f"""
        Task: Assign this bookmark to the most appropriate Category and Subcategory from the provided Taxonomy.
        
        [Input Bookmark]
        Title: {bm['user_title']}
        Tags: {tags}
        
        [Reference Taxonomy Tree]
        {taxonomy_str}
        
        [Rules]
        1. You MUST choose one Category (Level 1) and one Subcategory (Level 2) strictly from the Reference list.
        2. Do not invent new categories.
        3. Output format: "Category > Subcategory"
        """

        try:
            # Qwen 2.5 3B éå¸¸é€‚åˆè¿™ç§ "Selection" ä»»åŠ¡
            response = ollama.chat(model=MODEL_NAME, messages=[{'role': 'user', 'content': prompt}])
            result = response['message']['content'].strip()
            
            # ç®€å•çš„æ¸…æ´—
            result = result.split('\n')[0].replace('"', '').replace("'", "")
            
            if ">" in result:
                parts = result.split(">")
                bm['category'] = parts[0].strip()
                bm['subcategory'] = parts[1].strip()
            else:
                # å¦‚æœ AI åªè¾“å‡ºäº†ä¸€ä¸ªè¯ï¼Œå°è¯•åœ¨ taxonomy é‡Œæ‰¾å®ƒæ˜¯å±äºå“ªä¸ªå¤§ç±»çš„
                found = False
                for main, subs in custom_taxonomy.items():
                    if result in main:
                        bm['category'] = main
                        bm['subcategory'] = "General"
                        found = True
                        break
                    for sub in subs:
                        if result in sub:
                            bm['category'] = main
                            bm['subcategory'] = sub
                            found = True
                            break
                if not found:
                    bm['category'] = "Miscellaneous"
                    bm['subcategory'] = result
        except:
            bm['category'] = "Error"
            bm['subcategory'] = "Manual Review"

    # ä»¥å‰çš„ "Sub-step 4.2: åˆ†ç±»å‰ªæ" é€»è¾‘ä¾ç„¶å¯ä»¥ç”¨ï¼Œè¿™é‡Œçœç•¥ä»¥èŠ‚çœç¯‡å¹…ï¼Œå»ºè®®ä¿ç•™åŸæ–‡ä»¶ä¸­çš„é‚£éƒ¨åˆ†ä»£ç 
    
    with open(FILE_CATEGORIZED, 'w', encoding='utf-8') as f:
        json.dump(bookmarks, f, indent=2, ensure_ascii=False)
    print(f"âœ… [Step 4] å®Œæˆã€‚")

# --- æ¨¡å— 5: å»ºé€  (Export - Vivaldi/Chrome å…¼å®¹å¢å¼ºç‰ˆ) ---
def step5_export():
    import html  # [æ–°å¢] å¼•å…¥ HTML è½¬ä¹‰åº“
    
    print("ğŸš© [Step 5] ç”Ÿæˆ Vivaldi å…¼å®¹çš„ HTML ä¹¦ç­¾æ–‡ä»¶...")
    if not os.path.exists(FILE_CATEGORIZED): 
        print("âŒ æœªæ‰¾åˆ°åˆ†ç±»æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ Step 4")
        return
        
    with open(FILE_CATEGORIZED, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # æ„å»ºå†…å­˜ä¸­çš„åˆ†ç±»æ ‘
    tree = {}
    count = 0
    for item in data:
        # å…¼å®¹å¤„ç†
        main = item.get('category', 'Uncategorized').strip()
        sub = item.get('subcategory', 'General').strip()
        
        # ç®€å•çš„å±‚çº§æ¸…ç†
        if not main: main = "Uncategorized"
        if not sub: sub = "General"
        
        if main not in tree: tree[main] = {}
        if sub not in tree[main]: tree[main][sub] = []
        tree[main][sub].append(item)
        count += 1

    print(f"   -> å‡†å¤‡å¯¼å‡º {count} ä¸ªä¹¦ç­¾...")

    # å†™å…¥ Netscape æ ‡å‡†ä¹¦ç­¾æ ¼å¼ (ä¸¥æ ¼å…¼å®¹æ¨¡å¼)
    with open(OUTPUT_FINAL, 'w', encoding='utf-8') as f:
        # [å…³é”®ä¿®å¤ 1] æ·»åŠ æ ‡å‡†å¤´éƒ¨å’Œ META æ ‡ç­¾ï¼Œé˜²æ­¢æµè§ˆå™¨ä¹±ç æˆ–è§£æå¤±è´¥
        f.write('<!DOCTYPE NETSCAPE-Bookmark-file-1>\n')
        f.write('\n')
        f.write('<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=UTF-8">\n')
        f.write('<TITLE>AI Bookmarks</TITLE>\n')
        f.write('<H1>Bookmarks</H1>\n')
        f.write('<DL><p>\n')
        
        # æ’åºé€»è¾‘
        sorted_keys = sorted(tree.keys(), key=lambda x: (x == "Dead Links", x == "Unsorted Websites", x == "Websites by Domain", x))
        
        for main_cat in sorted_keys:
            # [å…³é”®ä¿®å¤ 2] å¯¹åˆ†ç±»åç§°ä¹Ÿè¿›è¡Œè½¬ä¹‰
            safe_main = html.escape(main_cat)
            f.write(f'    <DT><H3>{safe_main}</H3>\n')
            f.write('    <DL><p>\n')
            
            for sub_cat, items in sorted(tree[main_cat].items()):
                safe_sub = html.escape(sub_cat)
                f.write(f'        <DT><H3>{safe_sub}</H3>\n')
                f.write('        <DL><p>\n')
                
                for item in items:
                    url = item.get('url', '#')
                    title = item.get('user_title', 'Untitled')
                    
                    # [å…³é”®ä¿®å¤ 3] æ ¸å¿ƒï¼šå¯¹ URL å’Œæ ‡é¢˜è¿›è¡Œ HTML è½¬ä¹‰
                    # å¦‚æœä¸è½¬ä¹‰ï¼ŒURL é‡Œçš„ "&" æˆ–æ ‡é¢˜é‡Œçš„å¼•å·ä¼šå¯¼è‡´å¯¼å…¥ä¸­æ–­
                    safe_url = html.escape(url)
                    safe_title = html.escape(title)
                    
                    f.write(f'            <DT><A HREF="{safe_url}">{safe_title}</A>\n')
                
                f.write('        </DL><p>\n')
            f.write('    </DL><p>\n')
        f.write('</DL><p>\n')
            
    print(f"ğŸ‰ ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼æœ€ç»ˆæ–‡ä»¶å·²ç”Ÿæˆ: {OUTPUT_FINAL}")
    print("   -> ç°åœ¨è¯·å°è¯•å¯¼å…¥ Vivaldiï¼Œåº”è¯¥èƒ½æ˜¾ç¤ºå®Œæ•´ç›®å½•ç»“æ„äº†ã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # ä¿®æ”¹ç‚¹ 1ï¼šåœ¨ choices é‡ŒåŠ ä¸Š 35ï¼Œå¦åˆ™ç»ˆç«¯ä¼šæŠ¥é”™è¯´ "invalid choice"
    parser.add_argument('--step', type=int, choices=[1, 2, 3, 35, 4, 5])
    args = parser.parse_args()

    if args.step == 1: 
        step1_ingestion()
    elif args.step == 2: 
        step2_enrichment()
    elif args.step == 3: 
        step3_analysis()
    elif args.step == 35:  # ä¿®æ”¹ç‚¹ 2ï¼šç»‘å®šæ–°å†™çš„åˆ†ç±»æ ‘ç”Ÿæˆå‡½æ•°
        step3_5_taxonomy_gen()
    elif args.step == 4: 
        step4_categorization()
    elif args.step == 5: 
        step5_export()
